{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.loader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os, sys\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from git import Repo\n",
    "import re\n",
    "from pymemri.gitlab_api import MEMRI_PATH, MEMRI_GITLAB_BASE_URL, ACCESS_TOKEN_PATH, GITLAB_API_BASE_URL, TIME_FORMAT_GITLAB, \\\n",
    "PROJET_ID_PATTERN, DEFAULT_PACKAGE_VERSION, download_package_file, project_id_from_name, write_file_to_package_registry, \\\n",
    "get_registry_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULT_PLUGIN_MODEL_PACKAGE_NAME = \"plugin-model-package\"\n",
    "DEFAULT_PYTORCH_MODEL_NAME = \"pytorch_model.bin\"\n",
    "DEFAULT_HUGGINFACE_CONFIG_NAME = \"config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Downloading & Uploading functions for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_huggingface_model_to_package_registry(project_name, model, version=DEFAULT_PACKAGE_VERSION):\n",
    "    import torch\n",
    "    api_key = get_registry_api_key()\n",
    "    project_id = project_id_from_name(project_name, api_key)\n",
    "    local_save_dir = Path(\"/tmp\")\n",
    "    torch.save(model.state_dict(), local_save_dir / DEFAULT_PYTORCH_MODEL_NAME)\n",
    "    model.config.to_json_file(local_save_dir / DEFAULT_HUGGINFACE_CONFIG_NAME)\n",
    "    \n",
    "    for f in [DEFAULT_HUGGINFACE_CONFIG_NAME, DEFAULT_PYTORCH_MODEL_NAME]:\n",
    "        file_path = local_save_dir / f\n",
    "        print(f\"writing {f} to package registry of {project_name} with project id {project_id}\")\n",
    "        write_file_to_package_registry(project_id, file_path, api_key, package_name=DEFAULT_PLUGIN_MODEL_PACKAGE_NAME, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_model_to_package_registry(model, project_name=None):\n",
    "    project_name = project_name if project_name is not None else find_git_repo()\n",
    "    if type(model).__module__.startswith(\"transformers\"):\n",
    "        import transformers\n",
    "        import torch\n",
    "    if isinstance(model, transformers.PreTrainedModel):\n",
    "        write_huggingface_model_to_package_registry(project_name, model)\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not supported: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_huggingface_model_for_project(project_path=None, files=None, download_if_exists=False):\n",
    "    if files is None:\n",
    "        files = [\"config.json\", \"pytorch_model.bin\"]\n",
    "    for f in files:\n",
    "        out_file_path = download_package_file(f, project_path=project_path, package_name=DEFAULT_PLUGIN_MODEL_PACKAGE_NAME)\n",
    "    return out_file_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_huggingface_model_for_project(project_path=None, files=None, download_if_exists=False):\n",
    "    out_dir = download_huggingface_model_for_project(project_path, files, download_if_exists)\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(out_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Transformers tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing config.json to package registry of test-1234 with project id 190\n",
      "uploading /tmp/config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='100' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [100/100 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully uploaded /tmp/config.json\n",
      "writing pytorch_model.bin to package registry of test-1234 with project id 190\n",
      "uploading /tmp/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='100' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [100/100 04:23<00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully uploaded /tmp/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "write_model_to_package_registry(model, project_name=\"test-1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/koen/.memri/projects/finetuning-example/config.json\n",
      "downloading config.json from project memri/finetuning-example, package plugin-model-package\n",
      "writing config.json to /Users/koen/.memri/projects/finetuning-example\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin\n",
      "downloading pytorch_model.bin from project memri/finetuning-example, package plugin-model-package\n",
      "writing pytorch_model.bin to /Users/koen/.memri/projects/finetuning-example\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "model = load_huggingface_model_for_project(project_path=\"memri/finetuning-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/koen/.memri/projects/finetuning-example/config.json\n",
      "/Users/koen/.memri/projects/finetuning-example/config.json already exists, and `download_if_exists`==False, using cached version\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin already exists, and `download_if_exists`==False, using cached version\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "out_dir = download_huggingface_model_for_project(project_path=\"memri/finetuning-example\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(out_dir, num_labels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted basic.ipynb.\n",
      "Converted cvu.utils.ipynb.\n",
      "Converted data.dataset.ipynb.\n",
      "Converted data.loader.ipynb.\n",
      "Converted data.photo.ipynb.\n",
      "Converted exporters.exporters.ipynb.\n",
      "Converted gitlab_api.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted itembase.ipynb.\n",
      "Converted plugin.authenticators.credentials.ipynb.\n",
      "Converted plugin.authenticators.oauth.ipynb.\n",
      "Converted plugin.listeners.ipynb.\n",
      "Converted plugin.pluginbase.ipynb.\n",
      "Converted plugin.states.ipynb.\n",
      "Converted plugins.authenticators.password.ipynb.\n",
      "Converted pod.api.ipynb.\n",
      "Converted pod.client.ipynb.\n",
      "Converted pod.db.ipynb.\n",
      "Converted pod.utils.ipynb.\n",
      "Converted template.config.ipynb.\n",
      "Converted template.formatter.ipynb.\n",
      "Converted test_schema.ipynb.\n",
      "Converted test_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
