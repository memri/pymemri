{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.loader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os, sys\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "MEMRI_PATH = Path.home() / \".memri\"\n",
    "ACCESS_TOKEN_PATH = Path.home() / \".memri/access_token/access_token.txt\"\n",
    "GITLAB_API_BASE_URL = \"https://gitlab.memri.io/api/v4\"\n",
    "DEFAULT_PLUGIN_MODEL_PACKAGE_NAME = \"plugin-model-package\"\n",
    "DEFAULT_PYTORCH_MODEL_NAME = \"pytorch_model.bin\"\n",
    "DEFAULT_HUGGINFACE_CONFIG_NAME = \"config.json\"\n",
    "DEFAULT_PACKAGE_VERSION = \"0.0.1\"\n",
    "\n",
    "TIME_FORMAT_GITLAB = '%Y-%m-%dT%H:%M:%S.%fZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Downloading & Uploading functions for package registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_git_repo():\n",
    "    path = \".\"\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            repo = Repo(f\"{path + ('.' * i)}/\")\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "    if i == 9:\n",
    "        raise ValueError(f\"could not fine git repo in {os.path.abspath('')}\")\n",
    "    \n",
    "    repo_name = repo.remotes.origin.url.split('.git')[0].split('/')[-1]\n",
    "    return repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_registry_api_key():\n",
    "    ACCESS_TOKEN_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if ACCESS_TOKEN_PATH.is_file():\n",
    "        with open(ACCESS_TOKEN_PATH, \"r\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "        The first time you are uploading a model you need to create an access_token\n",
    "        at https://gitlab.memri.io/-/profile/personal_access_tokens?name=Model+Access+token&scopes=api\n",
    "        Click at the blue button with 'Create personal access token'\"\n",
    "        \"\"\")\n",
    "        access_token = getpass(\"Then copy your personal access token from 'Your new personal access token', and paste here: \")\n",
    "        with open(ACCESS_TOKEN_PATH, \"w\") as f:\n",
    "            f.write(access_token)\n",
    "        return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class upload_in_chunks(object):\n",
    "    def __init__(self, filename, chunksize=1 << 13):\n",
    "        self.filename = filename\n",
    "        self.chunksize = chunksize\n",
    "        self.totalsize = os.path.getsize(filename)\n",
    "        self.readsofar = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 1000\n",
    "        pb = progress_bar(range(n))\n",
    "        pb_iter = iter(pb)\n",
    "        i = 1\n",
    "        delta = 1 / n\n",
    "\n",
    "        with open(self.filename, 'rb') as file:\n",
    "            while True:\n",
    "                data = file.read(self.chunksize)\n",
    "                if not data:\n",
    "                    sys.stderr.write(\"\\n\")\n",
    "                    break\n",
    "                self.readsofar += len(data)\n",
    "                percent = self.readsofar * 1e2 / self.totalsize\n",
    "                while (percent / 100) > i * delta:\n",
    "                    next(pb_iter, None)\n",
    "                    i += 1\n",
    "                yield data\n",
    "        pb.update_bar(n)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.totalsize\n",
    "    \n",
    "class IterableToFileAdapter(object):\n",
    "    def __init__(self, iterable):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.length = len(iterable)\n",
    "\n",
    "    def read(self, size=-1): # TBD: add buffer for `len(data) > size` case\n",
    "        return next(self.iterator, b'')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_file_to_package_registry(project_id, file_path, api_key, version=DEFAULT_PACKAGE_VERSION):\n",
    "    file_path = Path(file_path)\n",
    "    file_name = file_path.name\n",
    "    \n",
    "    url = f\"{GITLAB_API_BASE_URL}/projects/{project_id}/packages/generic/{DEFAULT_PLUGIN_MODEL_PACKAGE_NAME}/{version}/{file_name}\"\n",
    "    print(f\"uploading {file_path}\")\n",
    "    it = upload_in_chunks(file_path, 10)\n",
    "    res = requests.put(url=url, data=IterableToFileAdapter(it), \n",
    "                     headers={\"PRIVATE-TOKEN\": api_key})\n",
    "    \n",
    "    if res.status_code not in [200, 201]:\n",
    "        print(f\"Failed to upload {file_path}: {res.content}\")\n",
    "    else:\n",
    "        print(f\"Succesfully uploaded {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def project_id_from_name(project_name, api_key):\n",
    "    res = requests.get(f\"{GITLAB_API_BASE_URL}/projects\",\n",
    "                       headers={\"PRIVATE-TOKEN\": api_key},\n",
    "                       params={\n",
    "                           \"owned\": True,\n",
    "                           \"search\": project_name\n",
    "                       })\n",
    "    res =  [x.get(\"id\") for x in res.json()]\n",
    "    if len(res) == 0:\n",
    "        raise ValueError(f\"No plugin found with name {project_name}\")\n",
    "    else:\n",
    "        return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_huggingface_model_to_package_registry(project_name, model):\n",
    "    import torch\n",
    "    api_key = get_registry_api_key()\n",
    "    project_id = project_id_from_name(project_name, api_key)\n",
    "    local_save_dir = Path(\"/tmp\")\n",
    "    torch.save(model.state_dict(), local_save_dir / DEFAULT_PYTORCH_MODEL_NAME)\n",
    "    model.config.to_json_file(local_save_dir / DEFAULT_HUGGINFACE_CONFIG_NAME)\n",
    "    \n",
    "    for f in [DEFAULT_HUGGINFACE_CONFIG_NAME, DEFAULT_PYTORCH_MODEL_NAME]:\n",
    "        file_path = local_save_dir / f\n",
    "        print(f\"writing {f} to package registry of {project_name} with project id {project_id}\")\n",
    "        write_file_to_package_registry(project_id, file_path, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_model_to_package_registry(model, project_name=None):\n",
    "    project_name = project_name if project_name is not None else find_git_repo()\n",
    "    if type(model).__module__.startswith(\"transformers\"):\n",
    "        import transformers\n",
    "        import torch\n",
    "    if isinstance(model, transformers.PreTrainedModel):\n",
    "        write_huggingface_model_to_package_registry(project_name, model)\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not supported: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_package_file(filename, project_name=None, out_dir=None, package_name=DEFAULT_PLUGIN_MODEL_PACKAGE_NAME,\n",
    "                          package_version=DEFAULT_PACKAGE_VERSION):\n",
    "    if project_name is None:\n",
    "        try:\n",
    "            project_name = find_git_repo()\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"no project name provided, but could also not find a git repo to infer project name\") from None\n",
    "    \n",
    "    out_dir = out_dir if out_dir is not None else MEMRI_PATH / \"projects\" / project_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    api_key = get_registry_api_key()\n",
    "    project_id = project_id_from_name(project_name, api_key)\n",
    "    print(f\"downloading {filename} from project {project_name}, package {package_name}\")\n",
    "\n",
    "    res = requests.get(\n",
    "        url=f\"{GITLAB_API_BASE_URL}/projects/{project_id}/packages/generic/{package_name}/{package_version}/{filename}\"\n",
    "    )\n",
    "    res.raise_for_status() \n",
    "    with open(out_dir / filename, \"wb\") as f:\n",
    "        print(f\"writing {filename} to {out_dir}\")\n",
    "        f.write(res.content)\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_huggingface_model_for_project(project=None, files=None):\n",
    "    if files is None:\n",
    "        files = [\"config.json\", \"pytorch_model.bin\"]\n",
    "    for f in files:\n",
    "        out_dir = download_package_file(f, project_name=project)\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading config.json from project finetuning-example, package plugin-model-package\n",
      "writing config.json to /Users/koen/.memri/projects/finetuning-example\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "# todo: cleanup old package files during testing\n",
    "filename = \"config.json\"\n",
    "out_file = download_package_file(filename, \"finetuning-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Transformers tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "write_model_to_package_registry(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading config.json from project finetuning-example, package plugin-model-package\n",
      "writing config.json to /Users/koen/.memri/projects/finetuning-example\n",
      "downloading pytorch_model.bin from project finetuning-example, package plugin-model-package\n",
      "writing pytorch_model.bin to /Users/koen/.memri/projects/finetuning-example\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "out_dir = download_huggingface_model_for_project(project=\"finetuning-example\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(out_dir, num_labels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted basic.ipynb.\n",
      "Converted cvu.utils.ipynb.\n",
      "Converted data.dataset.ipynb.\n",
      "Converted data.loader.ipynb.\n",
      "Converted data.photo.ipynb.\n",
      "Converted exporters.exporters.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted itembase.ipynb.\n",
      "Converted plugin.authenticators.credentials.ipynb.\n",
      "Converted plugin.authenticators.oauth.ipynb.\n",
      "Converted plugin.listeners.ipynb.\n",
      "Converted plugin.pluginbase.ipynb.\n",
      "Converted plugin.states.ipynb.\n",
      "Converted plugins.authenticators.password.ipynb.\n",
      "Converted pod.api.ipynb.\n",
      "Converted pod.client.ipynb.\n",
      "Converted pod.db.ipynb.\n",
      "Converted pod.utils.ipynb.\n",
      "Converted template.config.ipynb.\n",
      "Converted template.formatter.ipynb.\n",
      "Converted test_schema.ipynb.\n",
      "Converted test_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
