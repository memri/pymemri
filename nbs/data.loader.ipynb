{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.loader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os, sys\n",
    "from getpass import getpass\n",
    "from datetime import datetime\n",
    "from git import Repo\n",
    "import re\n",
    "from pymemri.gitlab_api import MEMRI_PATH, MEMRI_GITLAB_BASE_URL, ACCESS_TOKEN_PATH, GITLAB_API_BASE_URL, TIME_FORMAT_GITLAB, \\\n",
    "PROJET_ID_PATTERN, DEFAULT_PACKAGE_VERSION, GitlabAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULT_PLUGIN_MODEL_PACKAGE_NAME = \"plugin-model-package\"\n",
    "DEFAULT_PYTORCH_MODEL_NAME = \"pytorch_model.bin\"\n",
    "DEFAULT_HUGGINFACE_CONFIG_NAME = \"config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Downloading & Uploading functions for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_huggingface_model_to_package_registry(project_name, model, version=DEFAULT_PACKAGE_VERSION, client=None):\n",
    "    import torch\n",
    "    api = GitlabAPI(client=client)\n",
    "    project_id = api.project_id_from_name(project_name)\n",
    "    local_save_dir = Path(\"/tmp\")\n",
    "    torch.save(model.state_dict(), local_save_dir / DEFAULT_PYTORCH_MODEL_NAME)\n",
    "    model.config.to_json_file(local_save_dir / DEFAULT_HUGGINFACE_CONFIG_NAME)\n",
    "    \n",
    "    for f in [DEFAULT_HUGGINFACE_CONFIG_NAME, DEFAULT_PYTORCH_MODEL_NAME]:\n",
    "        file_path = local_save_dir / f\n",
    "        print(f\"writing {f} to package registry of {project_name} with project id {project_id}\")\n",
    "        api.write_file_to_package_registry(project_id, file_path, package_name=DEFAULT_PLUGIN_MODEL_PACKAGE_NAME, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def write_model_to_package_registry(model, project_name=None, client=None):\n",
    "    project_name = project_name if project_name is not None else find_git_repo()\n",
    "    if type(model).__module__.startswith(\"transformers\"):\n",
    "        import transformers\n",
    "        import torch\n",
    "    if isinstance(model, transformers.PreTrainedModel):\n",
    "        write_huggingface_model_to_package_registry(project_name, model, client=client)\n",
    "    else:\n",
    "        raise ValueError(f\"Model type not supported: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def download_huggingface_model_for_project(project_path=None, files=None, download_if_exists=False, client=None):\n",
    "    api = GitlabAPI(client=client)\n",
    "    if files is None:\n",
    "        files = [\"config.json\", \"pytorch_model.bin\"]\n",
    "    for f in files:\n",
    "        out_file_path = api.download_package_file(f, project_path=project_path, package_name=DEFAULT_PLUGIN_MODEL_PACKAGE_NAME)\n",
    "    return out_file_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_huggingface_model_for_project(project_path=None, files=None, download_if_exists=False, client=None):\n",
    "    out_dir = download_huggingface_model_for_project(project_path, files, download_if_exists, client=client)\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(out_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Transformers tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymemri.gitlab_api import GitlabAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = GitlabAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "write_model_to_package_registry(model, project_name=\"test-1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/koen/.memri/projects/finetuning-example/config.json\n",
      "/Users/koen/.memri/projects/finetuning-example/config.json already exists, and `download_if_exists`==False, using cached version\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin already exists, and `download_if_exists`==False, using cached version\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "model = load_huggingface_model_for_project(project_path=\"memri/finetuning-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/koen/.memri/projects/finetuning-example/config.json\n",
      "/Users/koen/.memri/projects/finetuning-example/config.json already exists, and `download_if_exists`==False, using cached version\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin\n",
      "/Users/koen/.memri/projects/finetuning-example/pytorch_model.bin already exists, and `download_if_exists`==False, using cached version\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "out_dir = download_huggingface_model_for_project(project_path=\"memri/finetuning-example\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(out_dir, num_labels=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted basic.ipynb.\n",
      "Converted cvu.utils.ipynb.\n",
      "Converted data.dataset.ipynb.\n",
      "Converted data.loader.ipynb.\n",
      "Converted data.oauth.ipynb.\n",
      "Converted data.photo.ipynb.\n",
      "Converted exporters.exporters.ipynb.\n",
      "Converted gitlab_api.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted itembase.ipynb.\n",
      "Converted plugin.authenticators.credentials.ipynb.\n",
      "Converted plugin.authenticators.oauth.ipynb.\n",
      "Converted plugin.listeners.ipynb.\n",
      "Converted plugin.pluginbase.ipynb.\n",
      "Converted plugin.states.ipynb.\n",
      "Converted plugins.authenticators.password.ipynb.\n",
      "Converted pod.api.ipynb.\n",
      "Converted pod.client.ipynb.\n",
      "Converted pod.db.ipynb.\n",
      "Converted pod.utils.ipynb.\n",
      "Converted template.config.ipynb.\n",
      "Converted template.formatter.ipynb.\n",
      "Converted test_schema.ipynb.\n",
      "Converted test_utils.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
